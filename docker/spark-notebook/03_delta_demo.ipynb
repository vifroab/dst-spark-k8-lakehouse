{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Delta Lake Core Features\n",
                "\n",
                "This notebook explores core **Delta Lake** features.\n",
                "\n",
                "**Key Features Covered:**\n",
                "1.  Table Creation\n",
                "2.  ACID Transactions\n",
                "3.  Time Travel\n",
                "4.  Schema Evolution\n",
                "5.  Vacuum"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import connector\n",
                "spark = connector.create_spark_session(\"delta-demo\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Create a Delta Table\n",
                "We'll create a table using the `delta` provider. Since we configured the default catalog to be DeltaCatalog, we can also use `spark_catalog`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We'll use an S3 path for the Delta table location\n",
                "table_path = \"s3a://polaris/delta/demo/events\"\n",
                "\n",
                "print(f\"Creating Delta table at {table_path}...\")\n",
                "spark.sql(f\"DROP TABLE IF EXISTS delta.`{table_path}`\")\n",
                "\n",
                "spark.sql(f\"\"\"\n",
                "    CREATE TABLE delta.`{table_path}` (\n",
                "        event_id BIGINT,\n",
                "        event_type STRING,\n",
                "        ts TIMESTAMP\n",
                "    )\n",
                "    USING delta\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ACID Transactions\n",
                "Delta supports Append and Overwrite modes with ACID guarantees."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Append Data (Version 1)\n",
                "print(\"Appending data (v1)...\")\n",
                "spark.sql(f\"\"\"\n",
                "    INSERT INTO delta.`{table_path}` VALUES\n",
                "    (1, 'login', TIMESTAMP '2023-01-01 10:00:00'),\n",
                "    (2, 'logout', TIMESTAMP '2023-01-01 10:30:00')\n",
                "\"\"\")\n",
                "spark.read.format(\"delta\").load(table_path).show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Overwrite Data (Version 2)\n",
                "print(\"Overwriting data (v2)...\")\n",
                "spark.sql(f\"\"\"\n",
                "    INSERT OVERWRITE delta.`{table_path}` VALUES\n",
                "    (3, 'purchase', TIMESTAMP '2023-01-02 12:00:00')\n",
                "\"\"\")\n",
                "spark.read.format(\"delta\").load(table_path).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Time Travel\n",
                "We can query previous versions of the table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show history\n",
                "spark.sql(f\"DESCRIBE HISTORY delta.`{table_path}`\").select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Query Version 1 (The Append)\n",
                "print(\"Querying Version 1...\")\n",
                "spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(table_path).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Schema Evolution\n",
                "Delta can automatically merge schema changes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
                "\n",
                "# Create a DataFrame with a NEW column 'user_id'\n",
                "data = [(4, 'click', 'user_123')]\n",
                "schema = StructType([\n",
                "    StructField(\"event_id\", LongType(), True),\n",
                "    StructField(\"event_type\", StringType(), True),\n",
                "    StructField(\"user_id\", StringType(), True) # New column\n",
                "])\n",
                "df = spark.createDataFrame(data, schema)\n",
                "\n",
                "print(\"Appending data with new column (mergeSchema)...\")\n",
                "df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(table_path)\n",
                "\n",
                "spark.read.format(\"delta\").load(table_path).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Vacuum\n",
                "Clean up old files to save space (removes ability to time travel beyond retention period)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# By default, vacuum retention is 7 days. We can override check for demo purposes.\n",
                "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
                "\n",
                "print(\"Vacuuming table (retain 0 hours)...\")\n",
                "spark.sql(f\"VACUUM delta.`{table_path}` RETAIN 0 HOURS\")\n",
                "\n",
                "# History is still there, but files for old versions might be gone\n",
                "spark.sql(f\"DESCRIBE HISTORY delta.`{table_path}`\").select(\"version\", \"operation\").show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Spark K8s (Python 3.12)",
            "language": "python",
            "name": "spark-k8s"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}