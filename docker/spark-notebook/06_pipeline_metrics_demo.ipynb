{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Data Pipeline with Metrics Tracking\n",
        "\n",
        "This notebook demonstrates a complete **Bronze ‚Üí Silver ‚Üí Gold** data pipeline with `dst_metrics` tracking at each layer.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "| Layer | Purpose | Metrics Logged |\n",
        "|-------|---------|----------------|\n",
        "| **Bronze** | Raw data ingestion | Files processed, rows loaded |\n",
        "| **Silver** | Data quality & cleaning | Null counts, duplicate counts, valid rows |\n",
        "| **Gold** | Business transformations | Join results, aggregations |\n",
        "\n",
        "All metrics are stored in a Delta table for auditing and monitoring.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared.spark_connector import SparkConnector\n",
        "from dst_metrics import df_count, df_avg, count_files\n",
        "from pyspark.sql.functions import col, when, count as spark_count, sum as spark_sum, lit\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# Create Spark session with S3/MinIO connectivity\n",
        "connector = SparkConnector(size=\"M\")\n",
        "spark = connector.session\n",
        "\n",
        "# Define storage paths (using MinIO S3)\n",
        "BRONZE_PATH = \"s3a://polaris/demo/bronze\"\n",
        "SILVER_PATH = \"s3a://polaris/demo/silver\"\n",
        "GOLD_PATH = \"s3a://polaris/demo/gold\"\n",
        "# Metrics automatically write to s3a://<bucket>/system/metrics/activity_log\n",
        "\n",
        "print(\"‚úÖ Spark session created\")\n",
        "print(f\"üìÅ Bronze: {BRONZE_PATH}\")\n",
        "print(f\"üìÅ Silver: {SILVER_PATH}\")\n",
        "print(f\"üìÅ Gold: {GOLD_PATH}\")\n",
        "print(f\"üìä Metrics: automatic (via connector.metric_context)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Create Sample Raw Data\n",
        "\n",
        "Let's create realistic sample data with some quality issues (nulls, duplicates) to demonstrate the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample Customers data (with some nulls and issues)\n",
        "customers_data = [\n",
        "    (1, \"Alice\", \"alice@example.com\", \"Copenhagen\", 28),\n",
        "    (2, \"Bob\", \"bob@example.com\", \"Aarhus\", 35),\n",
        "    (3, \"Charlie\", None, \"Odense\", 42),  # Missing email\n",
        "    (4, \"Diana\", \"diana@example.com\", None, 31),  # Missing city\n",
        "    (5, \"Eve\", \"eve@example.com\", \"Aalborg\", None),  # Missing age\n",
        "    (6, \"Frank\", \"frank@example.com\", \"Copenhagen\", 29),\n",
        "    (7, \"Grace\", \"grace@example.com\", \"Aarhus\", 45),\n",
        "    (1, \"Alice\", \"alice@example.com\", \"Copenhagen\", 28),  # Duplicate!\n",
        "    (8, \"Henry\", \"henry@example.com\", \"Odense\", 38),\n",
        "    (9, None, \"unknown@example.com\", \"Copenhagen\", 25),  # Missing name\n",
        "]\n",
        "\n",
        "customers_schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "# Sample Orders data\n",
        "orders_data = [\n",
        "    (101, 1, \"2024-01-15\", 150.00, \"completed\"),\n",
        "    (102, 2, \"2024-01-16\", 250.50, \"completed\"),\n",
        "    (103, 1, \"2024-01-17\", 75.00, \"completed\"),\n",
        "    (104, 3, \"2024-01-18\", 320.00, \"pending\"),\n",
        "    (105, 4, \"2024-01-19\", 180.00, \"completed\"),\n",
        "    (106, 5, \"2024-01-20\", 95.50, \"cancelled\"),\n",
        "    (107, 6, \"2024-01-21\", 420.00, \"completed\"),\n",
        "    (108, 2, \"2024-01-22\", 55.00, \"completed\"),\n",
        "    (109, 7, \"2024-01-23\", 290.00, \"pending\"),\n",
        "    (110, 8, \"2024-01-24\", 175.00, \"completed\"),\n",
        "]\n",
        "\n",
        "orders_schema = StructType([\n",
        "    StructField(\"order_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"order_date\", StringType(), True),\n",
        "    StructField(\"amount\", DoubleType(), True),\n",
        "    StructField(\"status\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Create DataFrames\n",
        "customers_raw = spark.createDataFrame(customers_data, customers_schema)\n",
        "orders_raw = spark.createDataFrame(orders_data, orders_schema)\n",
        "\n",
        "print(\"üìä Sample data created:\")\n",
        "print(f\"   Customers: {customers_raw.count()} rows\")\n",
        "print(f\"   Orders: {orders_raw.count()} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# ü•â Layer 1: Bronze (Raw Ingestion)\n",
        "\n",
        "The Bronze layer stores raw data exactly as received. We track:\n",
        "- Number of source files/records\n",
        "- Total rows ingested\n",
        "- Ingestion timestamp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with connector.metric_context(\n",
        "    layer=1,  # Bronze = Layer 1\n",
        "    project=\"demo\",\n",
        "    dataset_year=2024,\n",
        "    description=\"Bronze layer: Raw data ingestion\",\n",
        "    job_name=\"bronze_ingest\"\n",
        ") as ctx:\n",
        "    \n",
        "    # ========== CUSTOMERS ==========\n",
        "    customers_count = df_count(customers_raw)\n",
        "    \n",
        "    # Log: Rows ingested\n",
        "    ctx.log_metric(\n",
        "        layer=1, project=\"demo\", dataset_year=2024,\n",
        "        description=\"Customers rows ingested\",\n",
        "        value=customers_count,\n",
        "        unit=\"rows\",\n",
        "        function=\"count\",\n",
        "        job_name=\"bronze_customers_ingest\",\n",
        "        table_name=\"bronze_customers\",\n",
        "        source_path=\"raw/customers.csv\"\n",
        "    )\n",
        "    \n",
        "    # Write to Bronze\n",
        "    customers_raw.write.format(\"delta\").mode(\"overwrite\").save(f\"{BRONZE_PATH}/customers\")\n",
        "    print(f\"‚úÖ Bronze customers: {customers_count} rows written\")\n",
        "    \n",
        "    # ========== ORDERS ==========\n",
        "    orders_count = df_count(orders_raw)\n",
        "    \n",
        "    # Log: Rows ingested\n",
        "    ctx.log_metric(\n",
        "        layer=1, project=\"demo\", dataset_year=2024,\n",
        "        description=\"Orders rows ingested\",\n",
        "        value=orders_count,\n",
        "        unit=\"rows\",\n",
        "        function=\"count\",\n",
        "        job_name=\"bronze_orders_ingest\",\n",
        "        table_name=\"bronze_orders\",\n",
        "        source_path=\"raw/orders.csv\"\n",
        "    )\n",
        "    \n",
        "    # Write to Bronze\n",
        "    orders_raw.write.format(\"delta\").mode(\"overwrite\").save(f\"{BRONZE_PATH}/orders\")\n",
        "    print(f\"‚úÖ Bronze orders: {orders_count} rows written\")\n",
        "\n",
        "print(\"\\nü•â Bronze layer complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# ü•à Layer 2: Silver (Data Quality & Cleaning)\n",
        "\n",
        "The Silver layer handles data quality:\n",
        "- Remove duplicates\n",
        "- Handle nulls\n",
        "- Validate data types\n",
        "- Track quality metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with connector.metric_context(\n",
        "    layer=2,  # Silver = Layer 2\n",
        "    project=\"demo\",\n",
        "    dataset_year=2024,\n",
        "    description=\"Silver layer: Data quality & cleaning\",\n",
        "    job_name=\"silver_clean\"\n",
        ") as ctx:\n",
        "    \n",
        "    # ========== CUSTOMERS QUALITY CHECK ==========\n",
        "    customers_bronze = spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/customers\")\n",
        "    \n",
        "    # Count nulls per column\n",
        "    null_counts = customers_bronze.select([\n",
        "        spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) \n",
        "        for c in customers_bronze.columns\n",
        "    ]).collect()[0]\n",
        "    \n",
        "    total_nulls = sum([null_counts[c] for c in customers_bronze.columns])\n",
        "    \n",
        "    # Log: Null count\n",
        "    ctx.log_metric(\n",
        "        layer=2, project=\"demo\", dataset_year=2024,\n",
        "        description=\"Customers null values found\",\n",
        "        value=total_nulls,\n",
        "        unit=\"nulls\",\n",
        "        function=\"count\",\n",
        "        job_name=\"silver_customers_null_check\",\n",
        "        table_name=\"silver_customers\"\n",
        "    )\n",
        "    print(f\"‚ö†Ô∏è  Found {total_nulls} null values in customers\")\n",
        "    \n",
        "    # Count duplicates\n",
        "    total_rows = df_count(customers_bronze)\n",
        "    unique_rows = customers_bronze.dropDuplicates([\"customer_id\"]).count()\n",
        "    duplicate_count = total_rows - unique_rows\n",
        "    \n",
        "    # Log: Duplicate count\n",
        "    ctx.log_metric(\n",
        "        layer=2, project=\"demo\", dataset_year=2024,\n",
        "        description=\"Customers duplicates removed\",\n",
        "        value=duplicate_count,\n",
        "        unit=\"duplicates\",\n",
        "        function=\"count\",\n",
        "        job_name=\"silver_customers_dedup\",\n",
        "        table_name=\"silver_customers\"\n",
        "    )\n",
        "    print(f\"üîÑ Removed {duplicate_count} duplicate rows\")\n",
        "    \n",
        "    # Clean: Remove duplicates, fill nulls\n",
        "    customers_silver = (\n",
        "        customers_bronze\n",
        "        .dropDuplicates([\"customer_id\"])\n",
        "        .fillna({\"name\": \"Unknown\", \"email\": \"no-email@unknown.com\", \"city\": \"Unknown\", \"age\": 0})\n",
        "    )\n",
        "    \n",
        "    # Log: Clean rows\n",
        "    clean_count = df_count(customers_silver)\n",
        "    ctx.log_metric(\n",
        "        layer=2, project=\"demo\", dataset_year=2024,\n",
        "        description=\"Customers clean rows\",\n",
        "        value=clean_count,\n",
        "        unit=\"rows\",\n",
        "        function=\"count\",\n",
        "        job_name=\"silver_customers_output\",\n",
        "        table_name=\"silver_customers\"\n",
        "    )\n",
        "    \n",
        "    # Write to Silver\n",
        "    customers_silver.write.format(\"delta\").mode(\"overwrite\").save(f\"{SILVER_PATH}/customers\")\n",
        "    print(f\"‚úÖ Silver customers: {clean_count} clean rows written\")\n",
        "    \n",
        "    # ========== ORDERS QUALITY CHECK ==========\n",
        "    orders_bronze = spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/orders\")\n",
        "    \n",
        "    # Orders are clean in this demo, just pass through\n",
        "    orders_silver = orders_bronze\n",
        "    orders_clean_count = df_count(orders_silver)\n",
        "    \n",
        "    ctx.log_metric(\n",
        "        layer=2, project=\"demo\", dataset_year=2024,\n",
        "        description=\"Orders validated rows\",\n",
        "        value=orders_clean_count,\n",
        "        unit=\"rows\",\n",
        "        function=\"count\",\n",
        "        job_name=\"silver_orders_output\",\n",
        "        table_name=\"silver_orders\"\n",
        "    )\n",
        "    \n",
        "    orders_silver.write.format(\"delta\").mode(\"overwrite\").save(f\"{SILVER_PATH}/orders\")\n",
        "    print(f\"‚úÖ Silver orders: {orders_clean_count} rows written\")\n",
        "\n",
        "print(\"\\nü•à Silver layer complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# ü•á Layer 3: Gold (Business Transformations)\n",
        "\n",
        "The Gold layer creates business-ready datasets:\n",
        "- Join customers with orders\n",
        "- Calculate aggregations\n",
        "- Create summary tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with connector.metric_context(\n",
        "    layer=3,  # Gold = Layer 3\n",
        "    project=\"demo\",\n",
        "    dataset_year=2024,\n",
        "    description=\"Gold layer: Business transformations\",\n",
        "    job_name=\"gold_transform\"\n",
        ") as ctx:\n",
        "    \n",
        "    # Load Silver data\n",
        "    customers = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/customers\")\n",
        "    orders = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/orders\")\n",
        "    \n",
        "    # ========== JOIN: Customer Orders ==========\n",
        "    customer_orders = customers.join(orders, \"customer_id\", \"inner\")\n",
        "    join_count = df_count(customer_orders)\n",
        "    \n",
        "    ctx.log_metric(\n",
        "        layer=3, project=\"demo\", dataset_year=2024,\n",
        "        description=\"Customer-Orders join result\",\n",
        "        value=join_count,\n",
        "        unit=\"rows\",\n",
        "        function=\"count\",\n",
        "        job_name=\"gold_customer_orders_join\",\n",
        "        table_name=\"gold_customer_orders\"\n",
        "    )\n",
        "    \n",
        "    customer_orders.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/customer_orders\")\n",
        "    print(f\"‚úÖ Gold customer_orders: {join_count} rows\")\n",
        "    \n",
        "    # ========== AGGREGATION: Customer Summary ==========\n",
        "    from pyspark.sql.functions import sum as spark_sum, count as spark_count, avg as spark_avg\n",
        "    \n",
        "    customer_summary = (\n",
        "        customer_orders\n",
        "        .groupBy(\"customer_id\", \"name\", \"city\")\n",
        "        .agg(\n",
        "            spark_count(\"order_id\").alias(\"total_orders\"),\n",
        "            spark_sum(\"amount\").alias(\"total_spent\"),\n",
        "            spark_avg(\"amount\").alias(\"avg_order_value\")\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    summary_count = df_count(customer_summary)\n",
        "    \n",
        "    ctx.log_metric(\n",
        "        layer=3, project=\"demo\", dataset_year=2024,\n",
        "        description=\"Customer summary records\",\n",
        "        value=summary_count,\n",
        "        unit=\"customers\",\n",
        "        function=\"count\",\n",
        "        job_name=\"gold_customer_summary\",\n",
        "        table_name=\"gold_customer_summary\"\n",
        "    )\n",
        "    \n",
        "    # Log: Average order value (business metric)\n",
        "    avg_order = df_avg(customer_orders, \"amount\")\n",
        "    ctx.log_metric(\n",
        "        layer=3, project=\"demo\", dataset_year=2024,\n",
        "        description=\"Average order value\",\n",
        "        value=avg_order,\n",
        "        unit=\"DKK\",\n",
        "        function=\"avg\",\n",
        "        job_name=\"gold_avg_order_metric\",\n",
        "        table_name=\"gold_customer_orders\"\n",
        "    )\n",
        "    \n",
        "    # Log: Total revenue (business metric)\n",
        "    total_revenue = customer_orders.agg(spark_sum(\"amount\")).collect()[0][0]\n",
        "    ctx.log_metric(\n",
        "        layer=3, project=\"demo\", dataset_year=2024,\n",
        "        description=\"Total revenue\",\n",
        "        value=total_revenue,\n",
        "        unit=\"DKK\",\n",
        "        function=\"sum\",\n",
        "        job_name=\"gold_revenue_metric\",\n",
        "        table_name=\"gold_customer_orders\"\n",
        "    )\n",
        "    \n",
        "    customer_summary.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/customer_summary\")\n",
        "    print(f\"‚úÖ Gold customer_summary: {summary_count} customers\")\n",
        "    print(f\"\\nüí∞ Total Revenue: {total_revenue:.2f} DKK\")\n",
        "    print(f\"üìä Avg Order Value: {avg_order:.2f} DKK\")\n",
        "\n",
        "print(\"\\nü•á Gold layer complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üìà Pipeline Data Summary\n",
        "\n",
        "Let's see how the data transforms through each layer of the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ü•â BRONZE LAYER - Raw Data (with quality issues)\n",
        "# ============================================\n",
        "print(\"ü•â BRONZE: Raw Customers (notice nulls and duplicate customer_id=1)\")\n",
        "print(\"=\" * 80)\n",
        "bronze_customers = spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/customers\")\n",
        "bronze_customers.show(truncate=False)\n",
        "\n",
        "print(f\"\\nü•â BRONZE: Raw Orders\")\n",
        "print(\"=\" * 80)\n",
        "bronze_orders = spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/orders\")\n",
        "bronze_orders.show(truncate=False)\n",
        "\n",
        "# ============================================\n",
        "# ü•à SILVER LAYER - Cleaned Data\n",
        "# ============================================\n",
        "print(\"\\nü•à SILVER: Cleaned Customers (duplicates removed, nulls filled)\")\n",
        "print(\"=\" * 80)\n",
        "silver_customers = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/customers\")\n",
        "silver_customers.show(truncate=False)\n",
        "\n",
        "print(f\"\\nü•à SILVER: Validated Orders\")\n",
        "print(\"=\" * 80)\n",
        "silver_orders = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/orders\")\n",
        "silver_orders.show(truncate=False)\n",
        "\n",
        "# ============================================\n",
        "# ü•á GOLD LAYER - Business-Ready Aggregations\n",
        "# ============================================\n",
        "print(\"\\nü•á GOLD: Customer Orders (joined)\")\n",
        "print(\"=\" * 80)\n",
        "gold_orders = spark.read.format(\"delta\").load(f\"{GOLD_PATH}/customer_orders\")\n",
        "gold_orders.show(truncate=False)\n",
        "\n",
        "print(f\"\\nü•á GOLD: Customer Summary (aggregated)\")\n",
        "print(\"=\" * 80)\n",
        "gold_summary = spark.read.format(\"delta\").load(f\"{GOLD_PATH}/customer_summary\")\n",
        "gold_summary.orderBy(\"total_spent\", ascending=False).show(truncate=False)\n",
        "\n",
        "# ============================================\n",
        "# üìä Layer Row Counts Summary\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä PIPELINE SUMMARY - Row Counts\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"ü•â Bronze: {bronze_customers.count()} customers, {bronze_orders.count()} orders\")\n",
        "print(f\"ü•à Silver: {silver_customers.count()} customers, {silver_orders.count()} orders\")\n",
        "print(f\"ü•á Gold:   {gold_orders.count()} customer-orders, {gold_summary.count()} customer summaries\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üîç Analytics: Query the Metrics Table\n",
        "\n",
        "All metrics from the pipeline are stored in a Delta table. Let's query it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä All Pipeline Metrics:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Metrics are automatically stored by connector.metric_context()\n",
        "METRICS_PATH = connector.metrics_path  # e.g., s3a://polaris/system/metrics/activity_log\n",
        "print(f\"üìÅ Reading from: {METRICS_PATH}\")\n",
        "\n",
        "metrics_df = spark.read.format(\"delta\").load(METRICS_PATH)\n",
        "\n",
        "metrics_df.select(\n",
        "    \"event_timestamp\",\n",
        "    \"layer\",\n",
        "    \"job_name\",\n",
        "    \"description\",\n",
        "    \"metric_value\",\n",
        "    \"metric_unit\",\n",
        "    \"status\"\n",
        ").orderBy(\"event_timestamp\").show(50, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Metrics by Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nü•â BRONZE Layer Metrics:\")\n",
        "metrics_df.filter(col(\"layer\") == \"1\").select(\n",
        "    \"job_name\", \"description\", \"metric_value\", \"metric_unit\", \"table_name\"\n",
        ").show(truncate=False)\n",
        "\n",
        "print(\"\\nü•à SILVER Layer Metrics:\")\n",
        "metrics_df.filter(col(\"layer\") == \"2\").select(\n",
        "    \"job_name\", \"description\", \"metric_value\", \"metric_unit\", \"table_name\"\n",
        ").show(truncate=False)\n",
        "\n",
        "print(\"\\nü•á GOLD Layer Metrics:\")\n",
        "metrics_df.filter(col(\"layer\") == \"3\").select(\n",
        "    \"job_name\", \"description\", \"metric_value\", \"metric_unit\", \"table_name\"\n",
        ").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚è±Ô∏è Job Execution History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n‚è±Ô∏è Job Completion History:\")\n",
        "\n",
        "metrics_df.filter(col(\"metric_function\") == \"completion\").select(\n",
        "    \"event_timestamp\",\n",
        "    \"job_name\",\n",
        "    \"layer\",\n",
        "    \"status\",\n",
        "    \"duration_ms\"\n",
        ").orderBy(\"event_timestamp\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üéØ Summary\n",
        "\n",
        "This notebook demonstrated a complete **Bronze ‚Üí Silver ‚Üí Gold** data pipeline:\n",
        "\n",
        "| Layer | What Happened | Key Metrics |\n",
        "|-------|---------------|-------------|\n",
        "| ü•â **Bronze** | Raw data ingestion | 10 customers (with issues), 10 orders |\n",
        "| ü•à **Silver** | Data quality & cleaning | 4 nulls fixed, 1 duplicate removed ‚Üí 9 clean customers |\n",
        "| ü•á **Gold** | Business transformations | Customer-order joins, revenue aggregations |\n",
        "\n",
        "### Key `SparkConnector` Features Used:\n",
        "\n",
        "```python\n",
        "from shared.spark_connector import SparkConnector\n",
        "\n",
        "connector = SparkConnector(size=\"M\")\n",
        "spark = connector.session\n",
        "\n",
        "# Metrics write to s3a://<bucket>/system/metrics/activity_log automatically\n",
        "with connector.metric_context(\n",
        "    layer=3,\n",
        "    project=\"demo\",\n",
        "    dataset_year=2024,\n",
        "    description=\"Customer ETL\",\n",
        "    job_name=\"customer_etl\"\n",
        ") as ctx:\n",
        "    df = spark.read.csv(\"/path/to/data\")\n",
        "    ctx.log_metric(\n",
        "        layer=3, project=\"demo\", dataset_year=2024,\n",
        "        description=\"Rows loaded\",\n",
        "        value=df.count(),\n",
        "        unit=\"rows\",\n",
        "        function=\"count\"\n",
        "    )\n",
        "```\n",
        "\n",
        "### Storage Locations (MinIO S3):\n",
        "\n",
        "| Path | Contents |\n",
        "|------|----------|\n",
        "| `s3a://polaris/demo/bronze/` | Raw data (customers, orders) |\n",
        "| `s3a://polaris/demo/silver/` | Cleaned data |\n",
        "| `s3a://polaris/demo/gold/` | Business-ready aggregations |\n",
        "| `s3a://<bucket>/system/metrics/activity_log` | Pipeline metrics (automatic) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "connector.stop()\n",
        "print(\"üèÅ Demo complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
