{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Apache Iceberg Core Features\n",
                "\n",
                "This notebook explores core **Apache Iceberg** features using the Polaris catalog.\n",
                "\n",
                "**Key Features Covered:**\n",
                "1.  Table Creation (Partitioned)\n",
                "2.  ACID Operations (Insert, Update, Delete)\n",
                "3.  Schema Evolution\n",
                "4.  Time Travel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import connector\n",
                "spark = connector.create_spark_session(\"iceberg-demo\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Create a Partitioned Table\n",
                "We'll create a table `sales` partitioned by `day` (derived from `ts`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.sql(\"CREATE DATABASE IF NOT EXISTS polaris.sales_db\")\n",
                "spark.sql(\"DROP TABLE IF EXISTS polaris.sales_db.orders\")\n",
                "\n",
                "print(\"Creating partitioned table 'polaris.sales_db.orders'...\")\n",
                "spark.sql(\"\"\"\n",
                "    CREATE TABLE polaris.sales_db.orders (\n",
                "        order_id BIGINT,\n",
                "        customer_id BIGINT,\n",
                "        amount DOUBLE,\n",
                "        ts TIMESTAMP\n",
                "    )\n",
                "    USING iceberg\n",
                "    PARTITIONED BY (days(ts))\n",
                "\"\"\").show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ACID Operations\n",
                "Iceberg supports full ACID compliance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# INSERT\n",
                "print(\"Inserting data...\")\n",
                "spark.sql(\"\"\"\n",
                "    INSERT INTO polaris.sales_db.orders VALUES \n",
                "    (1, 100, 50.0, TIMESTAMP '2023-01-01 10:00:00'),\n",
                "    (2, 101, 25.5, TIMESTAMP '2023-01-01 11:00:00'),\n",
                "    (3, 100, 100.0, TIMESTAMP '2023-01-02 09:00:00')\n",
                "\"\"\")\n",
                "spark.table(\"polaris.sales_db.orders\").show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# UPDATE (Row-level update)\n",
                "print(\"Updating order 1 amount to 55.0...\")\n",
                "spark.sql(\"UPDATE polaris.sales_db.orders SET amount = 55.0 WHERE order_id = 1\")\n",
                "spark.table(\"polaris.sales_db.orders\").show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DELETE\n",
                "print(\"Deleting order 2...\")\n",
                "spark.sql(\"DELETE FROM polaris.sales_db.orders WHERE order_id = 2\")\n",
                "spark.table(\"polaris.sales_db.orders\").show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Schema Evolution\n",
                "Iceberg allows full schema evolution without rewriting data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Adding column 'discount' and renaming 'amount' to 'total_amount'...\")\n",
                "spark.sql(\"ALTER TABLE polaris.sales_db.orders ADD COLUMN discount DOUBLE\")\n",
                "spark.sql(\"ALTER TABLE polaris.sales_db.orders RENAME COLUMN amount TO total_amount\")\n",
                "\n",
                "spark.table(\"polaris.sales_db.orders\").printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Time Travel\n",
                "We can query the table as it existed in the past."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List snapshots to get IDs and timestamps\n",
                "spark.sql(\"SELECT committed_at, snapshot_id, operation, summary FROM polaris.sales_db.orders.snapshots\").show(truncate=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Query the first snapshot (before update/delete)\n",
                "# Replace the snapshot_id below with one from the output above manually if running interactively,\n",
                "# or we can programmatically fetch it:\n",
                "first_snapshot = spark.sql(\"SELECT snapshot_id FROM polaris.sales_db.orders.snapshots ORDER BY committed_at ASC LIMIT 1\").collect()[0][0]\n",
                "\n",
                "print(f\"Querying snapshot {first_snapshot}...\")\n",
                "spark.read.option(\"snapshot-id\", first_snapshot).table(\"polaris.sales_db.orders\").show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Spark K8s (Python 3.12)",
            "language": "python",
            "name": "spark-k8s"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
