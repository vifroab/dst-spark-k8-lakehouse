{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DST-Spark Template Notebook\n",
    "\n",
    "This notebook provides a template for connecting to Spark in two ways:\n",
    "1.  **Connecting to the shared DST-Spark cluster master.**\n",
    "2.  **Running a standalone local Spark session for testing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Connect to the DST-Spark Cluster\n",
    "\n",
    "This is the standard way to work. It uses the `spark_connector` module to connect your notebook to the shared Spark master node. All your computations will run on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Spark Master!\n",
      "Spark version: 4.0.0\n",
      "Spark master URL: spark://localhost:7077\n",
      "Number of cores: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             message|\n",
      "+--------------------+\n",
      "|Hello from Spark ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from spark_connector import get_spark_session\n",
    "\n",
    "# stop eksisterende session, ellers tager Spark ikke nye confs\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "\n",
    "# This function reads your environment variables and returns a\n",
    "\n",
    "# pre-configured Spark session connected to the master.\n",
    "# limited Spark session connected to the master.\n",
    "# Check with http://srvpython16:8080/#running-app after creation...\n",
    "#spark = get_spark_session()\n",
    "\n",
    "spark = get_spark_session(\n",
    "    cores_max=2,\n",
    "    executor_cores=1,\n",
    "    executor_memory=\"2g\",\n",
    "    memory_overhead=\"1g\",\n",
    "    dynamic_allocation=False\n",
    ")\n",
    "print(\"Successfully connected to Spark Master!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark master URL: {spark.conf.get('spark.master')}\")\n",
    "print(f\"Number of cores: {spark.sparkContext.getConf().get(\"spark.cores.max\")}\")\n",
    "\n",
    "# You can now use the 'spark' session for your work\n",
    "spark.sql(\"SELECT 'Hello from Spark Master!' as message\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Read and Write data from MinIO\n",
    "\n",
    "This example demonstrates reading and writing a Parquet file to the `shared` bucket in MinIO. You'll need write permissions for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Delta to s3a://shared/template_notebook_delta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/22 08:18:13 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "25/09/22 08:18:16 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write complete.\n",
      "Reading Delta from s3a://shared/template_notebook_delta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|      Bob| 45|\n",
      "|Catherine| 29|\n",
      "|    Alice| 34|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "s3_delta = \"s3a://shared/template_notebook_delta\"\n",
    "\n",
    "print(f\"Writing Delta to {s3_delta}...\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(s3_delta)\n",
    "print(\"Write complete.\")\n",
    "\n",
    "print(f\"Reading Delta from {s3_delta}...\")\n",
    "read_df = spark.read.format(\"delta\").load(s3_delta) #.show()\n",
    "read_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE PRIVATE\n",
    "s3_delta = \"s3a://home-victor/template_notebook_test_data\"\n",
    "\n",
    "\n",
    "print(f\"Writing Delta to {s3_delta}...\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(s3_delta)\n",
    "print(\"Write complete.\")\n",
    "\n",
    "print(f\"Reading Delta from {s3_delta}...\")\n",
    "read_df = spark.read.format(\"delta\").load(s3_delta).show()\n",
    "read_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE SOMEONE ELSES\n",
    "s3_delta = \"s3a://home-x11/silver/template_notebook_test_data\" # home-x11 => Thomas\n",
    "\n",
    "print(f\"Writing Delta to {s3_delta}...\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(s3_delta)\n",
    "print(\"Write complete.\")\n",
    "\n",
    "print(f\"Reading Delta from {s3_delta}...\")\n",
    "read_df = spark.read.format(\"delta\").load(s3_delta)  #.show()\n",
    "read_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Run a Local Spark Session\n",
    "\n",
    "This is useful for small tests or when you don't need the power of the cluster. This Spark session runs entirely inside the Jupyter container and does not connect to the Spark master or workers. It won't be able to access data on MinIO unless you configure it with S3 credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# This creates a completely local Spark session\n",
    "local_spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"LocalJupyterTest\")\n",
    "    .master(\"local[*]\") # Use all available local cores\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Successfully created a local Spark session!\")\n",
    "print(f\"Spark version: {local_spark.version}\")\n",
    "print(f\"Spark master URL: {local_spark.conf.get('spark.master')}\")\n",
    "\n",
    "# You can now use the 'local_spark' session\n",
    "local_spark.sql(\"SELECT 'Hello from Local Spark!' as message\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "It's good practice to stop your Spark sessions when you're finished to release resources on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the sessions when you're done\n",
    "print(\"Stopping Spark Master session...\")\n",
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "\n",
    "print(\"Stopping Local Spark session...\")\n",
    "if 'local_spark' in locals():\n",
    "    local_spark.stop()\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
