{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# SparkConnector Demonstration (k3d/JupyterHub + Spark-on-Kubernetes)\n",
    "\n",
    "This demo supports **two ways** to choose environment (`sbx/dev/test/prod`):\n",
    "\n",
    "1) **JupyterHub profile selection (recommended best practice)**\n",
    "   - The profile injects `DST_ENV`, `DST_BUCKET`, `POLARIS_WAREHOUSE`.\n",
    "\n",
    "2) **Git branch mapping (optional legacy behavior)**\n",
    "   - If a git repo is present in the notebook pod, this demo can map the current\n",
    "     branch to an environment and set the same env vars *before* Spark starts.\n",
    "\n",
    "Branch → Environment mapping used here:\n",
    "- `feature/*` (and unknown) → `sbx`\n",
    "- `dev` / `develop` → `dev`\n",
    "- `release/*` or `hotfix/*` → `test`\n",
    "- `main` / `master` → `prod`\n",
    "\n",
    "Notes:\n",
    "- MinIO credentials are **user credentials** (MINIO_USER/MINIO_PASSWORD) and are\n",
    "  provided at spawn time by the JupyterHub start form.\n",
    "- This file is mounted into the notebook pod at `/opt/leru/getting_started`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1) Verify LER-U mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utilities.__file__ = /opt/leru/utilities/__init__.py\n",
      "/opt/leru exists    = True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import utilities\n",
    "\n",
    "print(\"utilities.__file__ =\", utilities.__file__)\n",
    "print(\"/opt/leru exists    =\", os.path.exists(\"/opt/leru\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2) Optional: derive sbx/dev/test/prod from git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git_repo_dir = (none)\n",
      "git_branch   = None\n",
      "env_from_git = sbx\n",
      "DST_ENV          = sbx\n",
      "DST_BUCKET       = s3a://sbx\n",
      "POLARIS_WAREHOUSE= sbx\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _git_branch(repo_dir: str | None = None) -> str | None:\n",
    "    \"\"\"Return current git branch name, or None if not a git repo.\"\"\"\n",
    "    cmd = [\"git\"]\n",
    "    if repo_dir:\n",
    "        cmd += [\"-C\", repo_dir]\n",
    "    cmd += [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"]\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.DEVNULL).decode().strip()\n",
    "        return None if out in (\"\", \"HEAD\") else out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _branch_to_env(branch: str | None) -> str:\n",
    "    if not branch:\n",
    "        return \"sbx\"\n",
    "    if branch.startswith(\"feature/\"):\n",
    "        return \"sbx\"\n",
    "    if branch in (\"dev\", \"develop\"):\n",
    "        return \"dev\"\n",
    "    if branch.startswith((\"release/\", \"hotfix/\")):\n",
    "        return \"test\"\n",
    "    if branch in (\"main\", \"master\"):\n",
    "        return \"prod\"\n",
    "    return \"sbx\"\n",
    "\n",
    "\n",
    "# Choose where to look for git:\n",
    "# - If you have this repo cloned in the pod, prefer it\n",
    "# - Otherwise use current working directory\n",
    "repo_candidate = \"/home/jovyan/spark-k8-hub\"\n",
    "repo_dir = repo_candidate if Path(repo_candidate, \".git\").exists() else None\n",
    "\n",
    "branch = _git_branch(repo_dir)\n",
    "env_from_git = _branch_to_env(branch)\n",
    "\n",
    "print(\"git_repo_dir =\", repo_dir or \"(none)\")\n",
    "print(\"git_branch   =\", branch)\n",
    "print(\"env_from_git =\", env_from_git)\n",
    "\n",
    "# Toggle: set to True if you want git to override the JupyterHub profile env vars\n",
    "USE_GIT_FOR_ENV = True\n",
    "\n",
    "if USE_GIT_FOR_ENV:\n",
    "    os.environ[\"DST_ENV\"] = env_from_git\n",
    "    os.environ[\"DST_BUCKET\"] = f\"s3a://{env_from_git}\"\n",
    "    os.environ[\"POLARIS_WAREHOUSE\"] = env_from_git\n",
    "    # optional for debugging\n",
    "    if branch:\n",
    "        os.environ[\"DST_GIT_BRANCH\"] = branch\n",
    "\n",
    "print(\"DST_ENV          =\", os.environ.get(\"DST_ENV\"))\n",
    "print(\"DST_BUCKET       =\", os.environ.get(\"DST_BUCKET\"))\n",
    "print(\"POLARIS_WAREHOUSE=\", os.environ.get(\"POLARIS_WAREHOUSE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3) Create Spark session via SparkConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2beada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " CONFIGURING SPARK SESSION\n",
      "================================================================================\n",
      "  User:        root\n",
      "  Branch:      unknown\n",
      "  Environment: sbx\n",
      "  Bucket:      s3a://sbx\n",
      "  Size:        XS\n",
      "  Runtime:     kubernetes\n",
      "================================================================================\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.scratchdir\n",
      "25/12/16 11:16:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/16 11:16:24 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "25/12/16 11:16:24 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [s3a://sbx/spark-tmp/]. Please check your configured local directories.\n",
      "25/12/16 11:16:29 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " SPARK SESSION ACTIVE\n",
      "================================================================================\n",
      "  Environment:  sbx\n",
      "  Branch:       unknown\n",
      "  Bucket:       s3a://sbx\n",
      "  Size:         XS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- Connector env ---\n",
      "env_name      = sbx\n",
      "runtime       = kubernetes\n",
      "spark_master  = k8s://https://kubernetes.default.svc\n",
      "bucket        = s3a://sbx\n",
      "catalog_type  = in-memory\n"
     ]
    }
   ],
   "source": [
    "from utilities.spark_connector import SparkConnector\n",
    "\n",
    "connector = SparkConnector(size=\"XS\", force_new=True)\n",
    "spark = connector.session\n",
    "\n",
    "print(\"\\n--- Connector env ---\")\n",
    "print(\"env_name      =\", connector.env.env_name)\n",
    "print(\"runtime       =\", connector.env.runtime)\n",
    "print(\"spark_master  =\", connector.env.spark_master)\n",
    "print(\"bucket        =\", connector.env.bucket)\n",
    "print(\"catalog_type  =\", connector.env.catalog_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6400f7",
   "metadata": {},
   "source": [
    "## 4) Spark sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"count =\", spark.range(1000).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 5) Delta write/read to the selected bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Delta to: s3a://sbx/demo/connector_demonstrator_env_select/delta_table\n",
      "Read back:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/16 11:16:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 15:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  n|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "path = f\"{connector.env.bucket}/demo/connector_demonstrator_env_select/delta_table\"\n",
    "(\n",
    "    spark.range(10)\n",
    "    .withColumnRenamed(\"id\", \"n\")\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(path)\n",
    ")\n",
    "\n",
    "print(\"Wrote Delta to:\", path)\n",
    "print(\"Read back:\")\n",
    "spark.read.format(\"delta\").load(path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 6) Polaris/Iceberg smoke test (uses POLARIS_WAREHOUSE / catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/16 11:16:52 WARN AuthManagers: Inferring rest.auth.type=oauth2 since property credential was provided. Please explicitly set rest.auth.type to avoid this warning.\n",
      "25/12/16 11:16:52 WARN OAuth2Manager: Iceberg REST client is missing the OAuth2 server URI configuration and defaults to http://polaris.polaris.svc:8181/api/catalog/v1/oauth/tokens. This automatic fallback will be removed in a future Iceberg release.It is recommended to configure the OAuth2 endpoint using the 'oauth2-server-uri' property to be prepared. This warning will disappear if the OAuth2 endpoint is explicitly configured. See https://github.com/apache/iceberg/issues/10537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n",
      "✅ Polaris/Iceberg smoke test OK\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Catalog is always named \"polaris\" in Spark config, but warehouse/catalog name comes from env.\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS polaris.demo\").show()\n",
    "    spark.sql(\"DROP TABLE IF EXISTS polaris.demo.users\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE polaris.demo.users (\n",
    "            id INT,\n",
    "            name STRING\n",
    "        )\n",
    "        USING iceberg\n",
    "        \"\"\"\n",
    "    )\n",
    "    spark.sql(\"INSERT INTO polaris.demo.users VALUES (1, 'Alice'), (2, 'Bob')\")\n",
    "    spark.sql(\"SELECT * FROM polaris.demo.users\").show()\n",
    "    print(\"✅ Polaris/Iceberg smoke test OK\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Polaris/Iceberg smoke test skipped/failed:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 7) Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping Spark session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/16 11:16:56 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session stopped.\n",
      "Stopped Spark session\n"
     ]
    }
   ],
   "source": [
    "connector.stop()\n",
    "print(\"Stopped Spark session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da08001-edc6-47e8-aa9f-e9c452cb4847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags",
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "-widgets,-varInspector"
  },
  "kernelspec": {
   "display_name": "Spark K8s (Python 3.12)",
   "language": "python",
   "name": "spark-k8s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
