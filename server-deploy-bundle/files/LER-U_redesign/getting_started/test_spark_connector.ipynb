{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# SparkConnector Test Suite - Class-Based API\n",
    "\n",
    "Covers:\n",
    "- Class-based API and singleton pattern\n",
    "- Branch-based environment switching\n",
    "- HMS connectivity vs in-memory catalog\n",
    "- Delta Lake write/read and time travel\n",
    "- Resource configuration validation\n",
    "- Context manager\n",
    "\n",
    "Note: Tests create temp branches/databases/tables and clean up after themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "from pyspark.sql import Row\n",
    "from utilities.spark_connector import SparkConnector\n",
    "\n",
    "DEFAULT_TEST_SIZE = \"XS\"\n",
    "\n",
    "# --- helpers -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _git_current_branch() -> str:\n",
    "    try:\n",
    "        return (\n",
    "            subprocess.check_output(\n",
    "                [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], stderr=subprocess.DEVNULL\n",
    "            )\n",
    "            .strip()\n",
    "            .decode(\"utf-8\")\n",
    "        )\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def _git_checkout(branch: str) -> None:\n",
    "    subprocess.run(\n",
    "        [\"git\", \"checkout\", \"-b\", branch],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "    )\n",
    "    subprocess.run([\"git\", \"checkout\", branch], capture_output=True, check=True)\n",
    "\n",
    "\n",
    "def _drop_if_exists(spark, sql: str) -> None:\n",
    "    try:\n",
    "        spark.sql(sql)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# --- core branch test ---------------------------------------------------------\n",
    "\n",
    "\n",
    "def test_environment_for_branch(\n",
    "    branch_name: str, expected_env: str, expected_catalog: str\n",
    ") -> None:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üé¨ Testing Environment: {branch_name} ‚Üí {expected_env}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    connector = None\n",
    "    test_db = None\n",
    "    is_restricted = True  # Assume restricted by default for safety in cleanup\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nüîÑ Switching to branch '{branch_name}'...\")\n",
    "        _git_checkout(branch_name)\n",
    "        print(\"‚úì Branch switched\")\n",
    "\n",
    "        connector = SparkConnector(size=DEFAULT_TEST_SIZE, force_new=True)\n",
    "        spark = connector.session\n",
    "\n",
    "        print(\"\\nüìã Environment Validation:\")\n",
    "        print(f\"  Branch:      {connector.env.branch}\")\n",
    "        print(f\"  Environment: {connector.env.env_name}\")\n",
    "        print(f\"  Catalog:     {connector.env.catalog_type}\")\n",
    "        print(f\"  Bucket:      {connector.env.bucket}\")\n",
    "        print(f\"  HMS URI:     {connector.env.hms_uri or '(in-memory)'}\")\n",
    "\n",
    "        assert connector.env.env_name == expected_env, f\"Expected env '{expected_env}'\"\n",
    "        assert connector.env.catalog_type == expected_catalog, (\n",
    "            f\"Expected catalog '{expected_catalog}'\"\n",
    "        )\n",
    "\n",
    "        print(\"\\nüîç Catalog checks...\")\n",
    "        if connector.env.catalog_type == \"hive\":\n",
    "            hms_uri = spark.conf.get(\"spark.hadoop.hive.metastore.uris\", \"\")\n",
    "            assert hms_uri and \"thrift://\" in hms_uri, \"HMS URI must be set for Hive\"\n",
    "            spark.sql(\"SHOW DATABASES\").show()\n",
    "            print(\"  ‚úì HMS connectivity OK\")\n",
    "        else:\n",
    "            assert not spark.conf.get(\"spark.hadoop.hive.metastore.uris\", \"\"), (\n",
    "                \"HMS URI must be empty for in-memory\"\n",
    "            )\n",
    "            spark.sql(\"SHOW DATABASES\").show()\n",
    "            print(\"  ‚úì In-memory catalog OK\")\n",
    "\n",
    "        print(\"\\nüìù Write/Read checks...\")\n",
    "        username = connector.env.username\n",
    "        test_db = f\"{username}_test\"\n",
    "        sanitized_env = expected_env.replace(\"-\", \"_\")\n",
    "        test_table = f\"{test_db}.branch_test_{sanitized_env}\"\n",
    "        is_restricted = connector.env.env_name in [\"test\", \"prod\"]\n",
    "\n",
    "        if is_restricted:\n",
    "            print(\n",
    "                f\"  ‚Üí {connector.env.env_name.upper()}: read-only expected, skipping write\"\n",
    "            )\n",
    "            spark.sql(\"SHOW DATABASES\").collect()\n",
    "            print(\"  ‚úì Read OK\")\n",
    "        else:\n",
    "            # Create the database for any write-enabled environment\n",
    "            print(f\"  ‚Üí Creating temporary database '{test_db}'...\")\n",
    "            spark.sql(f\"CREATE DATABASE IF NOT EXISTS {test_db}\")\n",
    "            print(\"  ‚úì Database created.\")\n",
    "\n",
    "            df = spark.createDataFrame(\n",
    "                [(branch_name, expected_env, username)], [\"branch\", \"env\", \"user\"]\n",
    "            )\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(test_table)\n",
    "            print(f\"  ‚úì Wrote to table '{test_table}'\")\n",
    "            spark.read.table(test_table).show()\n",
    "            loc = (\n",
    "                spark.sql(f\"DESCRIBE DETAIL {test_table}\")\n",
    "                .select(\"location\")\n",
    "                .collect()[0][0]\n",
    "            )\n",
    "            assert connector.env.bucket.replace(\"s3a://\", \"\") in loc, (\n",
    "                \"Table not in expected bucket\"\n",
    "            )\n",
    "            print(f\"  ‚úì Location verified: {loc}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"‚úÖ ENVIRONMENT TEST PASSED: {branch_name} ‚Üí {expected_env}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    finally:\n",
    "        if connector and connector.is_active:\n",
    "            # If the environment was write-enabled, we created a database and must clean it up.\n",
    "            if not is_restricted and test_db:\n",
    "                print(f\"üßπ Cleaning up test database '{test_db}'...\")\n",
    "                _drop_if_exists(spark, f\"DROP DATABASE IF EXISTS {test_db} CASCADE\")\n",
    "                print(\"  ‚úì Cleanup complete.\")\n",
    "            connector.stop()\n",
    "\n",
    "\n",
    "# --- main sequence ------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    original_branch = _git_current_branch()\n",
    "    connector = None\n",
    "\n",
    "    try:\n",
    "        # 1) Resource sizes (Simplified to just acknowledge the test size)\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"üìä TEST 1: Running all tests with default size: {DEFAULT_TEST_SIZE}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # 2) Basic class API\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üß™ TEST 2: Class-Based API\")\n",
    "        print(\"=\" * 80)\n",
    "        connector = SparkConnector(size=DEFAULT_TEST_SIZE)\n",
    "        spark = connector.session\n",
    "        print(f\"Spark {spark.version}, app={spark.sparkContext.appName}\")\n",
    "        spark.sql(\"SELECT 'Class-based API works! üéâ' as message\").show()\n",
    "        print(\n",
    "            \"  env:\",\n",
    "            connector.env.env_name,\n",
    "            connector.env.catalog_type,\n",
    "            connector.env.bucket,\n",
    "        )\n",
    "\n",
    "        # 3) Singleton\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üß™ TEST 3: Singleton\")\n",
    "        print(\"=\" * 80)\n",
    "        c2 = SparkConnector(size=DEFAULT_TEST_SIZE)\n",
    "        print(\"same instance:\", connector is c2)\n",
    "        print(\"same session:\", connector.session is c2.session)\n",
    "\n",
    "        # 4) Env via spark.conf\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üß™ TEST 4: Env via spark.conf\")\n",
    "        print(\"=\" * 80)\n",
    "        assert spark.env.env_name == spark.conf.get(\"dst.env.env_name\")\n",
    "        assert spark.env.bucket == spark.conf.get(\"dst.env.bucket\")\n",
    "        print(\"  ‚úì spark.env and spark.conf match\")\n",
    "\n",
    "        # 5) Delta I/O basic\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üß™ TEST 5: Delta read/write\")\n",
    "        print(\"=\" * 80)\n",
    "        data = [\n",
    "            Row(id=1, name=\"Alice\", salary=120000),\n",
    "            Row(id=2, name=\"Bob\", salary=95000),\n",
    "            Row(id=3, name=\"Charlie\", salary=110000),\n",
    "        ]\n",
    "        df = spark.createDataFrame(data)\n",
    "        path = f\"{connector.env.bucket}/test/class_api_basic\"\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "        spark.read.format(\"delta\").load(path).show()\n",
    "        print(\"  ‚úì Delta I/O OK\")\n",
    "\n",
    "        # 6) Context manager\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üß™ TEST 6: Context manager\")\n",
    "        print(\"=\" * 80)\n",
    "        with SparkConnector(size=DEFAULT_TEST_SIZE) as sp:\n",
    "            sp.sql(\"SELECT 'Context manager OK' as msg\").show()\n",
    "\n",
    "        connector.stop()\n",
    "        connector = None\n",
    "\n",
    "        # 7-11) Branch-based environments\n",
    "        test_environment_for_branch(\"local/test-sandbox\", \"local-sandbox\", \"in-memory\")\n",
    "        test_environment_for_branch(\"feature/test-sbx\", \"sbx\", \"hive\")\n",
    "        test_environment_for_branch(\"develop\", \"dev\", \"hive\")\n",
    "        test_environment_for_branch(\"release/v1.0.0\", \"test\", \"hive\")\n",
    "        test_environment_for_branch(\"main\", \"prod\", \"hive\")\n",
    "\n",
    "        # 12) Delta versioning/time travel (on dev)\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üß™ TEST 12: Delta Versioning & Time Travel\")\n",
    "        print(\"=\" * 80)\n",
    "        _git_checkout(\"develop\")\n",
    "        connector = SparkConnector(size=DEFAULT_TEST_SIZE, force_new=True)\n",
    "        spark = connector.session\n",
    "\n",
    "        username = connector.env.username\n",
    "        test_db = f\"{username}_test\"\n",
    "        test_table = f\"{test_db}.version_test\"\n",
    "\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {test_db}\")\n",
    "\n",
    "        v0 = [(1, \"Product A\", 100), (2, \"Product B\", 200), (3, \"Product C\", 150)]\n",
    "        spark.createDataFrame(v0, [\"id\", \"product\", \"price\"]).write.format(\n",
    "            \"delta\"\n",
    "        ).mode(\"overwrite\").saveAsTable(test_table)\n",
    "        time.sleep(2)\n",
    "        v1 = [(1, \"Product A\", 120), (2, \"Product B\", 180), (4, \"Product D\", 250)]\n",
    "        spark.createDataFrame(v1, [\"id\", \"product\", \"price\"]).write.format(\n",
    "            \"delta\"\n",
    "        ).mode(\"overwrite\").saveAsTable(test_table)\n",
    "\n",
    "        latest = spark.read.table(test_table).orderBy(\"id\").collect()\n",
    "        print(\"LATEST:\", [dict(r.asDict()) for r in latest])\n",
    "\n",
    "        v0_read = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .option(\"versionAsOf\", 0)\n",
    "            .table(test_table)\n",
    "            .orderBy(\"id\")\n",
    "            .collect()\n",
    "        )\n",
    "        print(\"V0:\", [dict(r.asDict()) for r in v0_read])\n",
    "\n",
    "        assert len(latest) == 3 and len(v0_read) == 3\n",
    "        assert [r for r in latest if r[\"product\"] == \"Product A\"][0][\"price\"] == 120\n",
    "        assert [r for r in v0_read if r[\"product\"] == \"Product A\"][0][\"price\"] == 100\n",
    "        print(\"  ‚úì Time travel OK\")\n",
    "\n",
    "        _drop_if_exists(spark, f\"DROP TABLE IF EXISTS {test_table}\")\n",
    "        _drop_if_exists(spark, f\"DROP DATABASE IF EXISTS {test_db} CASCADE\")\n",
    "        connector.stop()\n",
    "        connector = None\n",
    "\n",
    "        # 13) Resource configuration validation\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üß™ TEST 13: Resource configuration validation\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        print(f\"  ‚Üí Verifying size: {DEFAULT_TEST_SIZE}\")\n",
    "        connector = SparkConnector(size=DEFAULT_TEST_SIZE, force_new=True)\n",
    "\n",
    "        conf = connector.session.sparkContext.getConf()\n",
    "        res = connector.env.resource\n",
    "\n",
    "        # Manually check against the expected XS values\n",
    "        assert res.size == \"XS\"\n",
    "        assert conf.get(\"spark.cores.max\") == \"2\"\n",
    "        assert conf.get(\"spark.executor.cores\") == \"1\"\n",
    "        assert conf.get(\"spark.executor.memory\") == \"2g\"\n",
    "\n",
    "        print(f\"  ‚úì {DEFAULT_TEST_SIZE} configuration applied correctly\")\n",
    "        connector.stop()\n",
    "        connector = None\n",
    "\n",
    "        # 14) Branch mismatch detection\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üß™ TEST 14: Branch mismatch warning\")\n",
    "        print(\"=\" * 80)\n",
    "        _git_checkout(\"develop\")\n",
    "        c1 = SparkConnector(size=DEFAULT_TEST_SIZE, force_new=True)\n",
    "        print(\"   Session branch:\", c1.env.branch)\n",
    "        _git_checkout(\"feature/branch-mismatch\")\n",
    "        c2 = SparkConnector(size=DEFAULT_TEST_SIZE)  # should warn\n",
    "        c2.restart()\n",
    "        print(\"   After restart branch:\", c2.env.branch)\n",
    "        c2.stop()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üéâ ALL TESTS COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    finally:\n",
    "        # Cleanup and return to original branch\n",
    "        if connector and connector.is_active:\n",
    "            connector.stop()\n",
    "        if original_branch and original_branch != \"unknown\":\n",
    "            try:\n",
    "                subprocess.run(\n",
    "                    [\"git\", \"checkout\", original_branch], capture_output=True\n",
    "                )\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "connector._session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags",
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "-widgets,-varInspector"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
